{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7512dba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\fahim\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\fahim\\anaconda3\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fahim\\anaconda3\\lib\\site-packages (4.62.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests) (2022.6.15.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e110a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f08c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully collected 0 posts!\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch data from Pushshift\n",
    "def fetch_reddit_posts(subreddit, total_posts=10000):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    posts = []\n",
    "    last_post_time = None  # Keeps track of the last post\n",
    "\n",
    "    while len(posts) < total_posts:\n",
    "        # Set parameters for API request\n",
    "        params = {\n",
    "            \"subreddit\": subreddit,\n",
    "            \"size\": 100,  # Fetch 100 posts per request\n",
    "            \"sort\": \"desc\",\n",
    "            \"before\": last_post_time,  # Fetch older posts\n",
    "            \"fields\": [\"title\", \"selftext\"]  # Only fetch title and body (selftext)\n",
    "        }\n",
    "\n",
    "        # API request\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json().get(\"data\", [])\n",
    "\n",
    "        if not data:  # Stop if no more posts are available\n",
    "            break\n",
    "\n",
    "        # Add data to list\n",
    "        for post in data:\n",
    "            posts.append({\n",
    "                \"Title\": post.get(\"title\", \"\"),\n",
    "                \"Body\": post.get(\"selftext\", \"\")\n",
    "            })\n",
    "\n",
    "        # Update the last post's timestamp for the next batch\n",
    "        last_post_time = data[-1][\"created_utc\"]\n",
    "\n",
    "        # Progress update\n",
    "        print(f\"Collected {len(posts)}/{total_posts} posts...\", end=\"\\r\")\n",
    "\n",
    "    return posts[:total_posts]  # Ensure we only return the required amount\n",
    "\n",
    "# Fetch 10,000 posts from r/depression\n",
    "subreddit_name = \"depression\"\n",
    "total_posts_needed = 10000\n",
    "posts_data = fetch_reddit_posts(subreddit_name, total_posts_needed)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(posts_data)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"reddit_depression_10000_posts.csv\", index=False)\n",
    "print(f\"\\n✅ Successfully collected {len(df)} posts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a91559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = \"askreddit\"  # Try with a different subreddit to check if the issue persists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0a81ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 403\n",
      "Response JSON: {'detail': 'Not authenticated'}\n",
      "No data returned, stopping.\n",
      "\n",
      "✅ Successfully collected 0 posts!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_reddit_posts(subreddit, total_posts=10000):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    posts = []\n",
    "    last_post_time = None\n",
    "\n",
    "    while len(posts) < total_posts:\n",
    "        params = {\n",
    "            \"subreddit\": subreddit,\n",
    "            \"size\": 100,\n",
    "            \"sort\": \"desc\",\n",
    "            \"before\": last_post_time,\n",
    "            \"fields\": [\"title\", \"selftext\"]\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Print the response status and content for debugging\n",
    "        print(\"Response Status Code:\", response.status_code)\n",
    "        print(\"Response JSON:\", response.json())  # Check the response JSON\n",
    "\n",
    "        data = response.json().get(\"data\", [])\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data returned, stopping.\")\n",
    "            break\n",
    "\n",
    "        for post in data:\n",
    "            posts.append({\n",
    "                \"Title\": post.get(\"title\", \"\"),\n",
    "                \"Body\": post.get(\"selftext\", \"\")\n",
    "            })\n",
    "\n",
    "        last_post_time = data[-1][\"created_utc\"]\n",
    "        print(f\"Collected {len(posts)}/{total_posts} posts...\", end=\"\\r\")\n",
    "\n",
    "    return posts[:total_posts]\n",
    "\n",
    "subreddit_name = \"depression\"\n",
    "total_posts_needed = 10000\n",
    "posts_data = fetch_reddit_posts(subreddit_name, total_posts_needed)\n",
    "\n",
    "df = pd.DataFrame(posts_data)\n",
    "\n",
    "df.to_csv(\"reddit_depression_10000_posts.csv\", index=False)\n",
    "print(f\"\\n✅ Successfully collected {len(df)} posts!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b77ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 403\n",
      "Response JSON: {'detail': 'Not authenticated'}\n",
      "No data returned, stopping.\n",
      "\n",
      "✅ Successfully collected 0 posts!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def fetch_reddit_posts(subreddit, total_posts=10000):\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    posts = []\n",
    "    last_post_time = None\n",
    "\n",
    "    # Your personal use script and secret key\n",
    "    client_id = \"ceOp-rWRx4mYjOwh6kyqYw\"  # Replace with your personal use script\n",
    "    client_secret = \"k97RMyzpV9LgMJTfPP-NEgHCmk9BPw\"  # Replace with your secret key\n",
    "\n",
    "    # Prepare headers with client ID and secret key\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "        'Authorization': f'Bearer {client_id}:{client_secret}'  # Include authorization header\n",
    "    }\n",
    "\n",
    "    while len(posts) < total_posts:\n",
    "        params = {\n",
    "            \"subreddit\": subreddit,\n",
    "            \"size\": 100,\n",
    "            \"sort\": \"desc\",\n",
    "            \"before\": last_post_time,\n",
    "            \"fields\": [\"title\", \"selftext\"]\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        # Check the status code and response content for debugging\n",
    "        print(\"Response Status Code:\", response.status_code)\n",
    "        print(\"Response JSON:\", response.json())  # Check the response JSON\n",
    "\n",
    "        data = response.json().get(\"data\", [])\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data returned, stopping.\")\n",
    "            break\n",
    "\n",
    "        for post in data:\n",
    "            posts.append({\n",
    "                \"Title\": post.get(\"title\", \"\"),\n",
    "                \"Body\": post.get(\"selftext\", \"\")\n",
    "            })\n",
    "\n",
    "        last_post_time = data[-1][\"created_utc\"]\n",
    "        print(f\"Collected {len(posts)}/{total_posts} posts...\", end=\"\\r\")\n",
    "\n",
    "    return posts[:total_posts]\n",
    "\n",
    "subreddit_name = \"depression\"\n",
    "total_posts_needed = 10000\n",
    "posts_data = fetch_reddit_posts(subreddit_name, total_posts_needed)\n",
    "\n",
    "df = pd.DataFrame(posts_data)\n",
    "\n",
    "df.to_csv(\"reddit_depression_10000_posts.csv\", index=False)\n",
    "print(f\"\\n✅ Successfully collected {len(df)} posts!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8995ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43c36d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Collecting prawcore<3,>=2.4\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting update_checker>=0.18\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting websocket-client>=0.54.0\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2022.6.15.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\fahim\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update-checker-0.18.0 websocket-client-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e346a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcba16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully collected 992 posts!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace these with your credentials\n",
    "client_id = \"ceOp-rWRx4mYjOwh6kyqYw\"\n",
    "client_secret = \"k97RMyzpV9LgMJTfPP-NEgHCmk9BPw\"\n",
    "user_agent = \"Aware-Albatross7813\"\n",
    "\n",
    "# Initialize Reddit API client\n",
    "reddit = praw.Reddit(client_id=client_id, \n",
    "                     client_secret=client_secret, \n",
    "                     user_agent=user_agent)\n",
    "\n",
    "# Define the subreddit and number of posts you want to collect\n",
    "subreddit_name = \"BingeEatingDisorder\"\n",
    "total_posts_needed = 10000\n",
    "\n",
    "# Function to fetch posts\n",
    "def fetch_reddit_posts(subreddit, total_posts):\n",
    "    posts = []\n",
    "    \n",
    "    # Fetch posts from the subreddit\n",
    "    for submission in reddit.subreddit(subreddit).new(limit=total_posts):\n",
    "        posts.append({\n",
    "            \"Title\": submission.title,\n",
    "            \"Body\": submission.selftext\n",
    "        })\n",
    "    \n",
    "    return posts\n",
    "\n",
    "# Collect the posts\n",
    "posts_data = fetch_reddit_posts(subreddit_name, total_posts_needed)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(posts_data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"reddit_Binge-Eating_Disorder_1000_posts.csv\", index=False)\n",
    "print(f\"✅ Successfully collected {len(df)} posts!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a734cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
